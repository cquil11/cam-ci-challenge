name: vLLM Latency

on:
  schedule:
    - cron: '0 * * * *' # top of each hour
  workflow_dispatch:
permissions:
  contents: write

jobs:
  benchmark-latency:
    runs-on: self-hosted
    # https://docs.github.com/en/actions/how-tos/write-workflows/choose-where-workflows-run/run-jobs-in-a-container
    container:
      image: vllm/vllm-openai:v0.10.1.1
      options: >-
        --gpus all
        --ipc=host
        -v /home/ubuntu/.cache/huggingface:/home/ubuntu/.cache/huggingface
        --entrypoint /bin/bash
      env:
        HUGGING_FACE_HUB_TOKEN: ${{ secrets.HF_TOKEN }}
        HF_HOME: /home/ubuntu/.cache/huggingface

    steps:
      - uses: actions/checkout@v4

      - name: Serve model
        shell: bash
        run: | # shell
          vllm serve meta-llama/Llama-3.1-8B-Instruct > vllm_server.log 2>&1 &
          VLLM_PID=$!
          
          echo "Started vLLM server with PID: $VLLM_PID"
          echo "Waiting for vLLM server to start..."
          
          for i in $(seq 1 60); do
            if curl -sf http://localhost:8000/v1/models; then
              echo "Server is ready!"
              break
            fi
            echo "Attempt $i: Server not ready yet, waiting 5 seconds..."
            sleep 5
          done
          
          curl http://localhost:8000/v1/models

      - name: Run basic benchmark
        shell: bash
        run: | # shell
          # Define input/output length pairs
          # input_lens=(300 500 800 1000 2000)
          # output_lens=(100 200 300 200 400)
          input_lens=(300 500)
          output_lens=(100 200)

          for i in "${!input_lens[@]}"; do
            input_len=${input_lens[$i]}
            output_len=${output_lens[$i]}
            # Let's assume a single H100 is capable of approx. 5k tps throughput
            # under reasonable latency for Llama 3.1 8B Instruct
            # Therefore we can calculate request rate roughly by 5000 / (input_len + output_len)
            request_rate=$(python3 -c "print(round(5000 / ($input_len + $output_len), 2))")
            # Now assume we want each benchmark to run for roughly 3 minutes, so we calculate
            # the number of total requests to be approx.  60 * 3 * request_rate
            run_duration_mins=3
            number_requests=$(python3 -c "import math; print(round(3 * 60 * $request_rate))")
            
            echo "Running benchmark: ${input_len} input, ${output_len} output, ${request_rate} RPS, ${number_requests} total requests"
            
            vllm bench serve \
              --backend vllm \
              --model meta-llama/Llama-3.1-8B-Instruct \
              --endpoint /v1/completions \
              --dataset-name random \
              --random-input-len ${input_len} \
              --random-output-len ${output_len} \
              --num-prompts ${number_requests} \
              --request-rate ${request_rate} \
              --save-result \
              --disable-tqdm \
              --result-filename "in_${input_len}_out_${output_len}.json" \
              --metric-percentiles 25,50,75,90,99,99.9
          done

      - name: Parse results and generate summary
        shell: bash
        run: | #python
          pip install tabulate
          python3 << 'EOF'
          import json
          import glob
          import re
          from tabulate import tabulate
          import os
          from datetime import datetime

          # Load existing data if there were previous runs or create file if it doesn't exist
          # yet (will only occur on the first round but just here for posterity)
          try:
              with open('data.json', 'r') as f:
                  timeseries_data = json.load(f)
          except FileNotFoundError:
              timeseries_data = {}

          current_time = datetime.utcnow().isoformat() + 'Z'
          
          # Define which metrics to track in timeseries
          performance_metrics_keys = [
              'total_token_throughput', 'request_throughput', 'output_throughput',
              'mean_ttft_ms', 'median_ttft_ms', 'std_ttft_ms', 
              'p25_ttft_ms', 'p50_ttft_ms', 'p75_ttft_ms', 'p90_ttft_ms', 'p99_ttft_ms', 'p99.9_ttft_ms',
              'mean_tpot_ms', 'median_tpot_ms', 'std_tpot_ms',
              'p25_tpot_ms', 'p50_tpot_ms', 'p75_tpot_ms', 'p90_tpot_ms', 'p99_tpot_ms', 'p99.9_tpot_ms',
              'mean_itl_ms', 'median_itl_ms', 'std_itl_ms',
              'p25_itl_ms', 'p50_itl_ms', 'p75_itl_ms', 'p90_itl_ms', 'p99_itl_ms', 'p99.9_itl_ms'
          ]
          
          results = []
          for file in sorted(glob.glob("in_*_out_*.json")):
              match = re.search(r'in_(\d+)_out_(\d+)', file)
              if match:
                  with open(file, 'r') as f:
                      data = json.load(f)
                  
                  input_len = int(match.group(1))
                  output_len = int(match.group(2))
                  
                  # Remove unwanted columns for display table these just clutter the summary table
                  unwanted = ['date', 'endpoint_type', 'label', 'tokenizer_id', 'burstiness', 'request_goodput']
                  display_data = {k: v for k, v in data.items() if k not in unwanted}
                  
                  ordered_data = {
                      'input_len': input_len,
                      'output_len': output_len,
                      'total_tokens': input_len + output_len
                  }
                  
                  for key, value in display_data.items():
                      if key not in ['input_len', 'output_len', 'total_tokens']:
                          ordered_data[key] = value
                  
                  results.append(ordered_data)
                  
                  config_key = f"{input_len}_{output_len}"
                  
                  if config_key not in timeseries_data:
                      timeseries_data[config_key] = {
                          'input_len': input_len,
                          'output_len': output_len,
                          'total_tokens': input_len + output_len,
                          'history': []
                      }
                  
                  # Extract only the performance metrics we want to track
                  performance_metrics = {}
                  for key in performance_metrics_keys:
                      if key in data:
                          performance_metrics[key] = data[key]
                  
                  timeseries_entry = {
                      'timestamp': current_time,
                      **performance_metrics
                  }
                  
                  timeseries_data[config_key]['history'].append(timeseries_entry)

          with open('data.json', 'w') as f:
              json.dump(timeseries_data, f, indent=2)

          # Create display table for the GH summary
          table = tabulate(results, headers="keys", tablefmt="github", floatfmt=".2f")

          summary_path = os.environ.get('GITHUB_STEP_SUMMARY')
          if summary_path:
              with open(summary_path, 'a') as f:
                  f.write("# vLLM Benchmark Results\n\n")
                  f.write(table)
                  f.write(f"\n\n**Data added to timeseries at {current_time}**\n")
          
          print(f"\nTimeseries data updated in data.json with {len(results)} new entries")
          EOF

      - name: Upload logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: vllm-logs-${{ github.run_number }}
          path: |
            *.log
            *.json      

      # - name: Merge all benchmark data
      #   run: python3 ${GITHUB_WORKSPACE}/.github/scripts/merge_data_points.py


      # - name: Configure Git safe directory
      #   run: git config --global --add safe.directory ${GITHUB_WORKSPACE}

      # - name: Commit data point
      #   run: |
      #     # idk why this problem exists
      #     # solution is here: https://stackoverflow.com/questions/72978485/git-submodule-update-failed-with-fatal-detected-dubious-ownership-in-reposit
      #     git config --global --add safe.directory ${GITHUB_WORKSPACE}
      #     git config --global user.name 'GitHub Action'
      #     git config --global user.email 'action@github.com'

      #     git add benchmark_history/
      #     git add data.json
      #     git commit -m "Add benchmark data for $(date -u)" || exit 0
      #     git push

      # - name: Upload results
      #   uses: actions/upload-artifact@v4
      #   if: always()
      #   with:
      #     name: latency-results-${{ github.run_number }}
      #     path: |
      #       *.json
      #       *.log

      # - name: Notify Slack on failure
      #   if: failure()
      #   uses: 8398a7/action-slack@v3
      #   with:
      #     status: failure
      #     channel: '#ext-cameron-semianalysis'
      #     text: 'Benchmark workflow failed'
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
