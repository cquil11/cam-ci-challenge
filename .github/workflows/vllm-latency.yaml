name: vLLM Latency

on:
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  benchmark-latency:
    # The type of runner that the job will run on
    runs-on: self-hosted
    # https://docs.github.com/en/actions/how-tos/write-workflows/choose-where-workflows-run/run-jobs-in-a-container
    container:
      image: vllm/vllm-openai:v0.10.1.1
      options: >-
        --gpus all
        --ipc=host
        -v /home/ubuntu/.cache/huggingface:/home/ubuntu/.cache/huggingface
        --entrypoint /bin/bash
      env:
        HUGGING_FACE_HUB_TOKEN: ${{ secrets.HF_TOKEN }}
        HF_HOME: /home/ubuntu/.cache/huggingface

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v4

      - name: Install script requirements
        run: pip install -r ${GITHUB_WORKSPACE}/.github/scripts/requirements.txt

      - name: Run latency benchmark
        shell: bash
        run: |
          for batch_size in {1..8}; do
            echo "Running batch size $batch_size..."
            vllm bench latency \
              --model meta-llama/Llama-3.1-8B-Instruct \
              --batch-size ${batch_size} \
              --input-len 512 \
              --output-len 128 \
              --output-json latency_bs${batch_size}.json \
              2>&1 | tee latency_bs${batch_size}.log
          done

      - name: Generate summary page
        run: |
          python3 ${GITHUB_WORKSPACE}/.github/scripts/latency_to_table.py --base-filename latency_bs --output-dir ${GITHUB_WORKSPACE}
          cat ${GITHUB_WORKSPACE}/table_markdown.md >> $GITHUB_STEP_SUMMARY

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: latency-results-${{ github.run_number }}
          path: |
            *.json
            *.log
