name: vLLM Latency

on:
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  benchmark-latency:
    # The type of runner that the job will run on
    runs-on: self-hosted
    # https://docs.github.com/en/actions/how-tos/write-workflows/choose-where-workflows-run/run-jobs-in-a-container
    container:
      image: vllm/vllm-openai:v0.10.1.1
      options: >-
        --gpus all
        --ipc=host
        -v /home/ubuntu/.cache/huggingface:/home/ubuntu/.cache/huggingface
        --entrypoint /bin/bash
      env:
        HUGGING_FACE_HUB_TOKEN: ${{ secrets.HF_TOKEN }}
        HF_HOME: /home/ubuntu/.cache/huggingface

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v4

      # # Runs a single command using the runners shell
      # - name: Serve model
      #   shell: bash
      #   run: |
      #     vllm serve meta-llama/Llama-3.1-8B-Instruct > vllm_server.log 2>&1 &
      #     VLLM_PID=$!
          
      #     echo "Started vLLM server with PID: $VLLM_PID"
      #     echo "Waiting for vLLM server to start..."
          
      #     for i in $(seq 1 60); do
      #       if curl -sf http://localhost:8000/v1/models; then
      #         echo "Server is ready!"
      #         break
      #       fi
      #       echo "Attempt $i: Server not ready yet, waiting 5 seconds..."
      #       sleep 5
      #     done
          
      #     curl http://localhost:8000/v1/models

      - name: Run latency benchmark
        shell: bash
        run: |
          for batch_size in {1..8}; do
            vllm bench latency \
              --model meta-llama/Llama-3.1-8B-Instruct \
              --batch-size ${batch_size} \
              --input-len 512 \
              --output-len 128 \
              --output-json latency_batch_bs${batch_size}.json
          done

      # - name: Upload logs
      #   uses: actions/upload-artifact@v4
      #   if: always()
      #   with:
      #     name: vllm-logs-${{ github.run_number }}
      #     path: |
      #       vllm_server.log
      #       benchmark_results.log
