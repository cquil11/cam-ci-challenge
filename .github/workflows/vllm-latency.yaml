name: vLLM Latency

on:
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  benchmark-latency:
    # The type of runner that the job will run on
    runs-on: self-hosted
    # https://docs.github.com/en/actions/how-tos/write-workflows/choose-where-workflows-run/run-jobs-in-a-container
    container:
      image: vllm/vllm-openai:v0.10.1.1
      options: >-
        --gpus all
        --ipc=host
        -v /home/ubuntu/.cache/huggingface:/root/.cache/huggingface
        --entrypoint /bin/bash
      env:
        HUGGING_FACE_HUB_TOKEN: ${{ secrets.HF_TOKEN }}

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v4

      # Runs a single command using the runners shell
      - name: Serve model
        shell: bash
        run: |
          vllm serve meta-llama/Llama-3.1-8B-Instruct | tee 2>&1 vllm_server.log &
          VLLM_PID=$!
          
          echo "Started vLLM server with PID: $VLLM_PID"
          echo "Waiting for vLLM server to start..."
          
          for i in $(seq 1 60); do
            if curl -sf http://localhost:8000/v1/models; then
              echo "Server is ready!"
              break
            fi
            echo "Attempt $i: Server not ready yet, waiting 5 seconds..."
            sleep 5
          done
          
          curl http://localhost:8000/v1/models

      - name: Run basic benchmark
        run: |
          # Download dataset
          wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json
          vllm bench serve --backend vllm --model meta-llama/Llama-3.1-8B-Instruct --endpoint /v1/completions --dataset-name sharegpt --dataset-path ${GITHUB_WORKSPACE}/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 10

